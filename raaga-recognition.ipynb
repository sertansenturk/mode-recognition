{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../ModeTonicEstimation/')\n",
    "\n",
    "from fileoperations.fileoperations import getFileNamesInDir\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from ModeTonicEstimation.Chordia import Chordia\n",
    "from ModeTonicEstimation import Evaluator as ev\n",
    "import ModeTonicEstimation.ModeFunctions as mf\n",
    "from ModeTonicEstimation import PitchDistribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I/O\n",
    "base_dir = '../../test_datasets/RagaRecognition_journal'\n",
    "tradition_dir = 'hindustani'  # possible: makam, carnatic, hindustani\n",
    "num_class_dir = '30_classes'  # possible: 5, 10, 15, 20, 25, 30\n",
    "\n",
    "data_dir = os.path.join(base_dir, tradition_dir)\n",
    "\n",
    "experiment_dir = os.path.join(base_dir, 'fileLists', tradition_dir, num_class_dir)\n",
    "experiment_file = getFileNamesInDir(experiment_dir, keyword='*.json')[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiments = json.load(open(experiment_file, 'r'))\n",
    "\n",
    "audio_names = [os.path.join(data_dir,e[2]) for e in experiments]\n",
    "pitch_files = [os.path.join(data_dir,e[2]+'.pitch') for e in experiments]\n",
    "pcd_files = [os.path.join(data_dir,e[2]+'.pcd') for e in experiments]\n",
    "audio_mbids = [e[0] for e in experiments]\n",
    "audio_tonics = [np.loadtxt(p+'.tonicFine') for p in audio_names]\n",
    "audio_labels = [e[1] for e in experiments]\n",
    "\n",
    "unique_labels = set(audio_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define feature extraction code: compute the pcd's of all recordings\n",
    "def getPCD(pitchfile, tonic_freq, source):\n",
    "    pitch_track = np.loadtxt(pitchfile)\n",
    "    if pitch_track.ndim > 1:  # assume the first col is time, the second is pitch and the rest is labels etc\n",
    "        pitch_track = pitch_track[:,1]\n",
    "\n",
    "    # Each chunk is converted to cents\n",
    "    pitch_cent = mf.hz_to_cent(pitch_track, ref_freq=tonic_freq)\n",
    "\n",
    "    # PitchDistribution of the current chunk is generated\n",
    "    pd = mf.generate_pd(pitch_cent, ref_freq=tonic_freq, \n",
    "                        smooth_factor=10.0, step_size=10.0,\n",
    "                        source=source)\n",
    "    return mf.generate_pcd(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCD extraction\n",
    "pcds = []\n",
    "for i, (pf, tf, am, al, pcdf) in enumerate(zip(pitch_files, audio_tonics, audio_mbids, audio_labels, pcd_files)):\n",
    "    print str(i) + \" Getting PCD of \" + am\n",
    "    \n",
    "    if not os.path.isfile(pcdf):\n",
    "        pcd_temp = getPCD(pf, tf, am)\n",
    "        pcd_temp.save(pcdf)\n",
    "        \n",
    "        pcds.append({'pcd': pcd_temp, 'label': al, 'audio_mbid': am})\n",
    "    else:\n",
    "        pcds.append({'pcd':PitchDistribution.load(pcdf), 'label': al, 'audio_mbid': am})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate objects\n",
    "che = Chordia(step_size=10, smooth_factor=15,\n",
    "                      chunk_size=0, threshold=0.5, \n",
    "                      overlap=0, frame_rate=196.0/44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx in range(0,len(audio_names)):\n",
    "    # divide training & testing (leave-one-out)\n",
    "    training_names = copy.deepcopy(audio_names)\n",
    "    training_pitch_files = copy.deepcopy(pitch_files)\n",
    "    training_pcd_files = copy.deepcopy(pcd_files)\n",
    "    training_pcds = copy.deepcopy(pcds)\n",
    "    training_mbids = copy.deepcopy(audio_mbids)\n",
    "    training_tonics = copy.deepcopy(audio_tonics)\n",
    "    training_labels = copy.deepcopy(audio_labels)\n",
    "\n",
    "    # pop the test audio from the training\n",
    "    test_name = training_names.pop(idx)\n",
    "    test_pitch_file = training_pitch_files.pop(idx)\n",
    "    test_pcd_file = training_pcd_files.pop(idx)\n",
    "    test_pcds = training_pcds.pop(idx)\n",
    "    test_mbid = training_mbids.pop(idx)\n",
    "    test_tonic = training_tonics.pop(idx)\n",
    "    test_label = training_labels.pop(idx)\n",
    "\n",
    "    model_save_dir = os.path.join(experiment_dir, 'chordia', test_mbid)\n",
    "    \n",
    "    # Training: get the pcds of each raga\n",
    "    models = dict()\n",
    "    for pcd_dict in training_pcds:\n",
    "        if pcd_dict['label'] not in models.keys():\n",
    "            models[pcd_dict['label']] = [pcd_dict['pcd']]\n",
    "        else:\n",
    "            models[pcd_dict['label']].append(pcd_dict['pcd'])\n",
    "            che.save_model(models[pcd_dict['label']], model_save_dir, pcd_dict['label'])\n",
    "            \n",
    "    # Testing \n",
    "    res = che.estimate(test_pitch_file, mode_names=list(unique_labels), \n",
    "                       mode_dir=model_save_dir, est_mode=True, \n",
    "                       distance_method='bhat',\n",
    "                       metric='pcd', tonic_freq=test_tonic)\n",
    "\n",
    "    print ', '.join([test_mbid, test_label, res[0]])\n",
    "    print res[0] == test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluator = ev.Evaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # get the data into appropriate format\n",
    "# [pitch_paths, pitch_base, pitch_fname] = fo.getFileNamesInDir(data_dir, '.pitch')\n",
    "# tonic_paths = [os.path.splitext(p)[0] + '.tonic' for p in pitch_paths]\n",
    "# mode_labels = []\n",
    "# for p in pitch_base:\n",
    "#     for r in modes:\n",
    "#         if r in p:\n",
    "#             mode_labels.append(r)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # make the data a single dictionary for housekeeping\n",
    "# data = []\n",
    "# for p, f, t, r in zip(pitch_paths, pitch_fname, tonic_paths, mode_labels):\n",
    "#     data.append({'file':p, 'name':os.path.splitext(f)[0],\n",
    "#                'tonic':float(np.loadtxt(t)), 'mode':r})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # experiments\n",
    "# results = dict()\n",
    "# for key, fold in folds.iteritems():\n",
    "#     # Training \n",
    "#     print key\n",
    "#     models = dict()\n",
    "#     for cur_mode in modes:\n",
    "#         [file_list, tonic_list] = zip(*[(rec['file'], rec['tonic']) for rec in fold['train']\n",
    "#                                         if rec['mode'] == cur_mode])\n",
    "#         models[cur_mode] = che.train(cur_mode, file_list, tonic_list, metric='pcd', \n",
    "#                                      save_dir = os.path.join(train_savefolder, key))\n",
    "                                     \n",
    "#     # Raag Recognition\n",
    "#     results[key] = []\n",
    "#     for rec in fold['test']:\n",
    "#         rec['pitch'] = np.loadtxt(rec['file'])[:,1]\n",
    "#         res = che.estimate(rec['pitch'], mode_names=modes, \n",
    "#                            mode_dir=os.path.join(train_savefolder, key), \n",
    "#                            est_tonic=False, est_mode=True, distance_method='bhat',\n",
    "#                            metric='pcd', ref_freq=rec['tonic'])[0]\n",
    "\n",
    "#         # evaluate\n",
    "#         results[key].append(evaluator.mode_evaluate(rec['file'], res, rec['mode']))\n",
    "\n",
    "#     print key + \": \" + str(100*np.mean([r['mode_eval'] for r in results[key]])) + '%'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
